{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.modules.distance import PairwiseDistance\n",
    "from datasets.LFWDataset import LFWDataset\n",
    "from datasets.AIHubDataset import AIHubDataset\n",
    "from losses.triplet_loss import TripletLoss\n",
    "from datasets.TripletLossDataset import TripletFaceDataset\n",
    "from validate_on_LFW import evaluate_lfw\n",
    "from validate_aihub import validate_aihub\n",
    "from plot import plot_roc_lfw, plot_accuracy_lfw\n",
    "from tqdm import tqdm\n",
    "from model.inceptionresnetv2 import InceptionResnetV2Triplet\n",
    "from model.mobilenetv2 import MobileNetV2Triplet\n",
    "from model.resnet import (\n",
    "    Resnet18Triplet,\n",
    "    Resnet34Triplet,\n",
    "    Resnet50Triplet,\n",
    "    Resnet101Triplet,\n",
    "    Resnet152Triplet,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_architecture(model_architecture, pretrained, embedding_dimension):\n",
    "    if model_architecture == \"resnet18\":\n",
    "        model = Resnet18Triplet(\n",
    "            embedding_dimension=embedding_dimension, pretrained=pretrained\n",
    "        )\n",
    "    elif model_architecture == \"resnet34\":\n",
    "        model = Resnet34Triplet(\n",
    "            embedding_dimension=embedding_dimension, pretrained=pretrained\n",
    "        )\n",
    "    elif model_architecture == \"resnet50\":\n",
    "        model = Resnet50Triplet(\n",
    "            embedding_dimension=embedding_dimension, pretrained=pretrained\n",
    "        )\n",
    "    elif model_architecture == \"resnet101\":\n",
    "        model = Resnet101Triplet(\n",
    "            embedding_dimension=embedding_dimension, pretrained=pretrained\n",
    "        )\n",
    "    elif model_architecture == \"resnet152\":\n",
    "        model = Resnet152Triplet(\n",
    "            embedding_dimension=embedding_dimension, pretrained=pretrained\n",
    "        )\n",
    "    elif model_architecture == \"inceptionresnetv2\":\n",
    "        model = InceptionResnetV2Triplet(\n",
    "            embedding_dimension=embedding_dimension, pretrained=pretrained\n",
    "        )\n",
    "    elif model_architecture == \"mobilenetv2\":\n",
    "        model = MobileNetV2Triplet(\n",
    "            embedding_dimension=embedding_dimension, pretrained=pretrained\n",
    "        )\n",
    "    print(\"Using {} model architecture.\".format(model_architecture))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model_gpu_mode(model):\n",
    "    flag_train_gpu = torch.cuda.is_available()\n",
    "    flag_train_multi_gpu = False\n",
    "\n",
    "    if flag_train_gpu and torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "        model.cuda()\n",
    "        flag_train_multi_gpu = True\n",
    "        print(\"Using multi-gpu training.\")\n",
    "\n",
    "    elif flag_train_gpu and torch.cuda.device_count() == 1:\n",
    "        model.cuda()\n",
    "        print(\"Using single-gpu training.\")\n",
    "\n",
    "    return model, flag_train_multi_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_optimizer(optimizer, model, learning_rate):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer_model = optim.SGD(\n",
    "            params=model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            momentum=0.9,\n",
    "            dampening=0,\n",
    "            nesterov=False,\n",
    "            weight_decay=1e-5,\n",
    "        )\n",
    "\n",
    "    elif optimizer == \"adagrad\":\n",
    "        optimizer_model = optim.Adagrad(\n",
    "            params=model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            lr_decay=0,\n",
    "            initial_accumulator_value=0.1,\n",
    "            eps=1e-10,\n",
    "            weight_decay=1e-5,\n",
    "        )\n",
    "\n",
    "    elif optimizer == \"rmsprop\":\n",
    "        optimizer_model = optim.RMSprop(\n",
    "            params=model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            alpha=0.99,\n",
    "            eps=1e-08,\n",
    "            momentum=0,\n",
    "            centered=False,\n",
    "            weight_decay=1e-5,\n",
    "        )\n",
    "\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer_model = optim.Adam(\n",
    "            params=model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-08,\n",
    "            amsgrad=False,\n",
    "            weight_decay=1e-5,\n",
    "        )\n",
    "\n",
    "    return optimizer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_lfw(model, lfw_dataloader, model_architecture, epoch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        l2_distance = PairwiseDistance(p=2)\n",
    "        distances, labels = [], []\n",
    "\n",
    "        print(\"Validating on LFW! ...\")\n",
    "        progress_bar = enumerate(tqdm(lfw_dataloader))\n",
    "\n",
    "        for batch_index, (data_a, data_b, label) in progress_bar:\n",
    "            data_a = data_a.cuda()\n",
    "            data_b = data_b.cuda()\n",
    "\n",
    "            output_a, output_b = model(data_a), model(data_b)\n",
    "            distance = l2_distance.forward(output_a, output_b)  # Euclidean distance\n",
    "\n",
    "            distances.append(distance.cpu().detach().numpy())\n",
    "            labels.append(label.cpu().detach().numpy())\n",
    "\n",
    "        labels = np.array([sublabel for label in labels for sublabel in label])\n",
    "        distances = np.array(\n",
    "            [subdist for distance in distances for subdist in distance]\n",
    "        )\n",
    "\n",
    "        (\n",
    "            true_positive_rate,\n",
    "            false_positive_rate,\n",
    "            precision,\n",
    "            recall,\n",
    "            accuracy,\n",
    "            roc_auc,\n",
    "            best_distances,\n",
    "            tar,\n",
    "            far,\n",
    "        ) = evaluate_lfw(distances=distances, labels=labels, far_target=1e-3)\n",
    "        # Print statistics and add to log\n",
    "        print(\n",
    "            \"Accuracy on LFW: {:.4f}+-{:.4f}\\tPrecision {:.4f}+-{:.4f}\\tRecall {:.4f}+-{:.4f}\\t\"\n",
    "            \"ROC Area Under Curve: {:.4f}\\tBest distance threshold: {:.2f}+-{:.2f}\\t\"\n",
    "            \"TAR: {:.4f}+-{:.4f} @ FAR: {:.4f}\".format(\n",
    "                np.mean(accuracy),\n",
    "                np.std(accuracy),\n",
    "                np.mean(precision),\n",
    "                np.std(precision),\n",
    "                np.mean(recall),\n",
    "                np.std(recall),\n",
    "                roc_auc,\n",
    "                np.mean(best_distances),\n",
    "                np.std(best_distances),\n",
    "                np.mean(tar),\n",
    "                np.std(tar),\n",
    "                np.mean(far),\n",
    "            )\n",
    "        )\n",
    "        with open(\"logs/lfw_{}_log_triplet.txt\".format(model_architecture), \"a\") as f:\n",
    "            val_list = [\n",
    "                epoch,\n",
    "                np.mean(accuracy),\n",
    "                np.std(accuracy),\n",
    "                np.mean(precision),\n",
    "                np.std(precision),\n",
    "                np.mean(recall),\n",
    "                np.std(recall),\n",
    "                roc_auc,\n",
    "                np.mean(best_distances),\n",
    "                np.std(best_distances),\n",
    "                np.mean(tar),\n",
    "            ]\n",
    "            log = \"\\t\".join(str(value) for value in val_list)\n",
    "            f.writelines(log + \"\\n\")\n",
    "\n",
    "    try:\n",
    "        # Plot ROC curve\n",
    "        plot_roc_lfw(\n",
    "            false_positive_rate=false_positive_rate,\n",
    "            true_positive_rate=true_positive_rate,\n",
    "            figure_name=\"plots/roc_plots/roc_{}_epoch_{}_triplet.png\".format(\n",
    "                model_architecture, epoch\n",
    "            ),\n",
    "        )\n",
    "        # Plot LFW accuracies plot\n",
    "        plot_accuracy_lfw(\n",
    "            log_file=\"logs/aihub_{}_log_triplet.txt\".format(model_architecture),\n",
    "            epochs=epoch,\n",
    "            dataset=\"AIHub\",\n",
    "            figure_name=\"plots/accuracies_plots/aihub_accuracies_{}_epoch_{}_triplet.png\".format(\n",
    "                model_architecture, epoch\n",
    "            ),\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    return best_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(imgs, model, batch_size):\n",
    "    imgs = imgs.cuda()\n",
    "    embeddings = model(imgs)\n",
    "\n",
    "    # Split the embeddings into Anchor, Positive, and Negative embeddings\n",
    "    anc_embeddings = embeddings[:batch_size]\n",
    "    pos_embeddings = embeddings[batch_size : batch_size * 2]\n",
    "    neg_embeddings = embeddings[batch_size * 2 :]\n",
    "\n",
    "    return anc_embeddings, pos_embeddings, neg_embeddings, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = \"data/face-image/train_aihub_family\"\n",
    "lfw_dataroot = \"data/face-image/lfw_224\"\n",
    "aihub_dataroot = \"data/face-image/valid_aihub_family\"\n",
    "training_dataset_csv_path = \"aihub_train.csv\"\n",
    "epochs = 2  # 150\n",
    "iterations_per_epoch = 10  # 5000\n",
    "model_architecture = \"resnet34\"\n",
    "pretrained = True  # False\n",
    "embedding_dimension = 512\n",
    "num_human_identities_per_batch = 32\n",
    "batch_size = 100  # 544\n",
    "lfw_batch_size = 200\n",
    "resume_path = \"\"\n",
    "num_workers = 1\n",
    "optimizer = \"adagrad\"\n",
    "learning_rate = 0.075\n",
    "margin = 0.2\n",
    "image_size = 140\n",
    "use_semihard_negatives = False\n",
    "training_triplets_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_training_triplets_path = False\n",
    "start_epoch = 0\n",
    "\n",
    "if training_triplets_path is not None:\n",
    "    flag_training_triplets_path = (\n",
    "        True  # Load triplets file for the first training epoch\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image data pre-processing transforms\n",
    "#   ToTensor() normalizes pixel values between [0, 1]\n",
    "#   Normalize(mean=[0.6071, 0.4609, 0.3944], std=[0.2457, 0.2175, 0.2129]) according to the calculated glint360k\n",
    "#   dataset with tightly-cropped faces dataset RGB channels' mean and std values by\n",
    "#   calculate_glint360k_rgb_mean_std.py in 'datasets' folder.\n",
    "aihub_mean = [0.5444, 0.4335, 0.3800]\n",
    "aihub_std = [0.2672, 0.2295, 0.2156]\n",
    "data_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(size=(image_size, image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(degrees=5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=aihub_mean, std=aihub_std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "lfw_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(size=(image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.6071, 0.4609, 0.3944], std=[0.2457, 0.2175, 0.2129]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "aihub_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(size=(image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=aihub_mean, std=aihub_std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=LFWDataset(\n",
    "        dir=lfw_dataroot, pairs_path=\"datasets/LFW_pairs.txt\", transform=lfw_transforms\n",
    "    ),\n",
    "    batch_size=lfw_batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "aihub_dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=AIHubDataset(\n",
    "        dir=aihub_dataroot,\n",
    "        pairs_path=\"data/pairs/valid/pairs_Family.txt\",\n",
    "        transform=aihub_transforms,\n",
    "    ),\n",
    "    batch_size=lfw_batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = set_model_architecture(\n",
    "    model_architecture=model_architecture,\n",
    "    pretrained=pretrained,\n",
    "    embedding_dimension=embedding_dimension,\n",
    ")\n",
    "\n",
    "# Load model to GPU or multiple GPUs if available\n",
    "model, flag_train_multi_gpu = set_model_gpu_mode(model)\n",
    "\n",
    "# Set optimizer\n",
    "optimizer_model = set_optimizer(\n",
    "    optimizer=optimizer, model=model, learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume from a model checkpoint\n",
    "if resume_path:\n",
    "    if os.path.isfile(resume_path):\n",
    "        print(\"Loading checkpoint {} ...\".format(resume_path))\n",
    "        checkpoint = torch.load(resume_path)\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        optimizer_model.load_state_dict(checkpoint[\"optimizer_model_state_dict\"])\n",
    "\n",
    "        # In order to load state dict for optimizers correctly, model has to be loaded to gpu first\n",
    "        if flag_train_multi_gpu:\n",
    "            model.module.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        print(\"Checkpoint loaded: start epoch from checkpoint = {}\".format(start_epoch))\n",
    "    else:\n",
    "        print(\n",
    "            \"WARNING: No checkpoint found at {}!\\nTraining from scratch.\".format(\n",
    "                resume_path\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_semihard_negatives:\n",
    "    print(\"Using Semi-Hard negative triplet selection!\")\n",
    "else:\n",
    "    print(\"Using Hard negative triplet selection!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = start_epoch\n",
    "\n",
    "print(\n",
    "    \"Training using triplet loss starting for {} epochs:\\n\".format(epochs - start_epoch)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, epochs):\n",
    "    num_valid_training_triplets = 0\n",
    "    l2_distance = PairwiseDistance(p=2)\n",
    "    _training_triplets_path = None\n",
    "\n",
    "    if flag_training_triplets_path:\n",
    "        _training_triplets_path = training_triplets_path\n",
    "        flag_training_triplets_path = (\n",
    "            False  # Only load triplets file for the first epoch\n",
    "        )\n",
    "\n",
    "    # Re-instantiate training dataloader to generate a triplet list for this training epoch\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=TripletFaceDataset(\n",
    "            root_dir=dataroot,\n",
    "            training_dataset_csv_path=training_dataset_csv_path,\n",
    "            num_triplets=iterations_per_epoch * batch_size,\n",
    "            num_human_identities_per_batch=num_human_identities_per_batch,\n",
    "            triplet_batch_size=batch_size,\n",
    "            epoch=epoch,\n",
    "            training_triplets_path=_training_triplets_path,\n",
    "            transform=data_t nnnnnnnnnnnnnnnnnnnnransforms,\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False,  # Shuffling for triplets with set amount of human identities per batch is not required\n",
    "    )\n",
    "\n",
    "    # Training pass\n",
    "    model.train()\n",
    "    # progress_bar = enumerate(tqdm(train_dataloader))\n",
    "    progress_bar = enumerate(train_dataloader)\n",
    "\n",
    "    for batch_idx, (batch_sample) in progress_bar:\n",
    "\n",
    "        # Forward pass - compute embeddings\n",
    "        anc_imgs = batch_sample[\"anc_img\"]\n",
    "        pos_imgs = batch_sample[\"pos_img\"]\n",
    "        neg_imgs = batch_sample[\"neg_img\"]\n",
    "\n",
    "        # Concatenate the input images into one tensor because doing multiple forward passes would create\n",
    "        #  weird GPU memory allocation behaviours later on during training which would cause GPU Out of Memory\n",
    "        #  issues\n",
    "        all_imgs = torch.cat(\n",
    "            (anc_imgs, pos_imgs, neg_imgs)\n",
    "        )  # Must be a tuple of Torch Tensors\n",
    "\n",
    "        anc_embeddings, pos_embeddings, neg_embeddings, model = forward_pass(\n",
    "            imgs=all_imgs, model=model, batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        pos_dists = l2_distance.forward(anc_embeddings, pos_embeddings)\n",
    "        neg_dists = l2_distance.forward(anc_embeddings, neg_embeddings)\n",
    "\n",
    "        if use_semihard_negatives:\n",
    "            # Semi-Hard Negative triplet selection\n",
    "            #  (negative_distance - positive_distance < margin) AND (positive_distance < negative_distance)\n",
    "            #   Based on: https://github.com/davidsandberg/facenet/blob/master/src/train_tripletloss.py#L295\n",
    "            first_condition = (neg_dists - pos_dists < margin).cpu().numpy().flatten()\n",
    "            second_condition = (pos_dists < neg_dists).cpu().numpy().flatten()\n",
    "            all = np.logical_and(first_condition, second_condition)\n",
    "            valid_triplets = np.where(all == 1)\n",
    "        else:\n",
    "            # Hard Negative triplet selection\n",
    "            #  (negative_distance - positive_distance < margin)\n",
    "            #   Based on: https://github.com/davidsandberg/facenet/blob/master/src/train_tripletloss.py#L296\n",
    "            all = (neg_dists - pos_dists < margin).cpu().numpy().flatten()\n",
    "            valid_triplets = np.where(all == 1)\n",
    "\n",
    "        anc_valid_embeddings = anc_embeddings[valid_triplets]\n",
    "        pos_valid_embeddings = pos_embeddings[valid_triplets]\n",
    "        neg_valid_embeddings = neg_embeddings[valid_triplets]\n",
    "\n",
    "        # Calculate triplet loss\n",
    "        triplet_loss = TripletLoss(margin=margin).forward(\n",
    "            anchor=anc_valid_embeddings,\n",
    "            positive=pos_valid_embeddings,\n",
    "            negative=neg_valid_embeddings,\n",
    "        )\n",
    "\n",
    "        # Calculating number of triplets that met the triplet selection method during the epoch\n",
    "        num_valid_training_triplets += len(anc_valid_embeddings)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer_model.zero_grad()\n",
    "        triplet_loss.backward()\n",
    "        optimizer_model.step()\n",
    "    \n",
    "    # Print training statistics for epoch and add to log\n",
    "    print('Epoch {}:\\tNumber of valid training triplets in epoch: {}'.format(\n",
    "            epoch,\n",
    "            num_valid_training_triplets\n",
    "        )\n",
    "    )\n",
    "\n",
    "    with open('logs/{}_log_triplet.txt'.format(model_architecture), 'a') as f:\n",
    "        val_list = [\n",
    "            epoch,\n",
    "            num_valid_training_triplets\n",
    "        ]\n",
    "        log = '\\t'.join(str(value) for value in val_list)\n",
    "        f.writelines(log + '\\n')\n",
    "\n",
    "    # Evaluation pass on LFW dataset\n",
    "    is_aihub = True\n",
    "    validate = validate_aihub if is_aihub else validate_lfw\n",
    "    loader = aihub_dataloader if is_aihub else lfw_dataloader\n",
    "    best_distances = validate(\n",
    "        model=model,\n",
    "        aihub_dataloader=loader,\n",
    "        model_architecture=model_architecture,\n",
    "        epoch=epoch\n",
    "    )\n",
    "\n",
    "    # Save model checkpoint\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'embedding_dimension': embedding_dimension,\n",
    "        'batch_size_training': batch_size,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_architecture': model_architecture,\n",
    "        'optimizer_model_state_dict': optimizer_model.state_dict(),\n",
    "        'best_distance_threshold': np.mean(best_distances)\n",
    "    }\n",
    "\n",
    "    # For storing data parallel model's state dictionary without 'module' parameter\n",
    "    if flag_train_multi_gpu:\n",
    "        state['model_state_dict'] = model.module.state_dict()\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save(state, 'model_training_checkpoints/model_{}_triplet_epoch_{}.pt'.format(\n",
    "            model_architecture,\n",
    "            epoch\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
